import math
import pytest
import torch

from triton_int4.quant_layer import Int4PackedLinear, replace_linear_with_int4
from triton_int4.triton_kernels.quant_i4_pack8 import (dequantize_i4_pack8, quantize_i4_pack8)
from triton_int4.triton_kernels.matmul_bf16_i4_pack8 import matmul_bf16_i4

@pytest.fixture(scope="module")
def device():
    if not torch.cuda.is_available():
        pytest.skip("CUDA required")
    return "cuda"

def test_memory_reduction(device):
    x = torch.randn(64, 512, dtype=torch.float16, device=device)
    packed, _ = quantize_i4_pack8(x)
    orig_bytes = x.numel() * x.element_size()
    packed_bytes = packed.numel() * packed.element_size()
    assert math.isclose(packed_bytes / orig_bytes, 0.25, rel_tol=1e-3)

def test_roundtrip_accuracy(device):
    x = torch.randn(32, 256, dtype=torch.float16, device=device)
    packed, scales = quantize_i4_pack8(x)
    restored = dequantize_i4_pack8(packed, scales)
    x_fp32 = x.to(torch.float32)
    diff = (x_fp32 - restored).abs().mean()
    assert diff < 0.05

def test_int4_linear_layer(device):
    linear = torch.nn.Linear(64, 128, bias=True, device=device, dtype=torch.float16)
    x = torch.randn(32, 64, dtype=torch.float16, device=device)
    ref = linear(x).to(torch.float32)
    packed = Int4PackedLinear(linear)
    out = packed(x)
    assert out.shape == ref.shape
    assert (out - ref).abs().max() < 1.0

def test_replace_linear_recursively(device):
    class Toy(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.ffn = torch.nn.Sequential(
                torch.nn.Linear(32, 64, bias=False, device=device, dtype=torch.float16),
                torch.nn.Linear(64, 16, bias=False, device=device, dtype=torch.float16),
            )
        def forward(self, x):
            return self.ffn(x)
    model = Toy().to(device)
    replace_linear_with_int4(model)
    for module in model.modules():
        if module is model:
            continue
        assert not isinstance(module, torch.nn.Linear)